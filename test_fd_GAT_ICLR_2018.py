import glob
import re

import sys
import os

from torch.nn.modules.transformer import TransformerDecoderLayer
from torch_geometric.nn.conv.message_passing import MessagePassing
from torch_geometric.nn import global_mean_pool

sys.path.append(os.path.join(os.path.dirname(__file__), "dataset"))
sys.path.append(os.path.join(os.path.dirname(__file__), "model"))

import numpy as np
from multiprocessing import Pool
from functools import reduce
from tqdm import tqdm

from torch_geometric.nn import GATConv, GCNConv, GatedGraphConv
from torch_geometric.data import Data, InMemoryDataset
from torch_geometric.loader import DataLoader

from feature import *
from coders import *
from sampling import sample_random_order

from factbase import FactBase, Constant
from nutils import *
from semantics import FactBaseSemanticsDataset

from snapshot import ModelSnapshot
from bgp_semantics import BgpSemantics

import argparse

NUM_EDGE_TYPES = 4
NUM_TRAINING_EPOCH = 400
# NUM_LEN_DATASET = 1024
# NUM_LEN_DATASET_EVAL = max(100, int(NUM_LEN_DATASET*0.25))
NUM_LEN_DATASET_EVAL = 100

# determines factbase consistency
# (input: Semantic checker and network configuration data)
# (output: Summary of network configuration consistency check results)
def check_factbase(semantics, data):
    program = FactBase.from_data(data) # transfer the data from "PyG" to "FactBase"
    _, summary = semantics.check(program, ignore_missing_fwd_facts=True, return_summary=True) # FactBase â†’ NetworkX â†’ Prot()
    return summary

def combine_dict(dicts):
    flatten = lambda seq: reduce(lambda a,b: a.union(b), seq, set())
    keys = flatten([set(d.keys()) for d in dicts])
    values_for_key = lambda k: [d[k] for d in dicts if k in d.keys()]
    return dict([(k,torch.tensor(values_for_key(k))) for k in keys])

class AsyncFactBaseChecker: # multi-process of check_factbase();
    def __init__(self, pool):
        self.checking_tasks = []
        self.current_step = -1
        self.on_step_finish = None

        self.pool = pool

    def wait_for_step_results(self, global_step):
        if self.current_step == global_step:
            return
        else:
            if any([not task.ready() for task in self.checking_tasks]):
                print("info: waiting for program checker in evaluation (consider using more processes)")

            self._get_results_blocking()

            self.current_step = global_step
            self.checking_tasks = []

    def _get_results_blocking(self):
        summaries = [task.get() for task in self.checking_tasks]
        results = combine_dict(summaries)
        if self.current_step != -1:
            assert self.on_step_finish is not None, "must set self.on_step_finish callback function"
            self.on_step_finish(self.current_step, results)

    def check_last_step_results(self):
        if any([not task.ready() for task in self.checking_tasks]):
            return
        self._get_results_blocking()

    def queue(self, semantics, data, global_step):
        self.wait_for_step_results(global_step)
        self.checking_tasks.append(self.pool.apply_async(check_factbase, (semantics, data)))

class MaxGraphLayer(MessagePassing):
    def __init__(self, hidden_dim):
        super().__init__(aggr='max')

        self.lin = torch.nn.Linear(hidden_dim, hidden_dim)
        self.message_func = torch.nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x, edge_index):
        x = self.lin(x)
        x = torch.relu(x)

        return self.propagate(edge_index, x=x)

    def message(self, x_j):
        return torch.relu(self.message_func.forward(x_j))

class EdgeTypeTransformerLayer(torch.nn.Module): #
    def __init__(self, hidden_dim, d_inner, num_edge_types, n_heads=8, dropout=0.2):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.d_inner = d_inner

        # initialise layers
        self.layers = []
        for j in range(num_edge_types):
            l = GATConv(self.hidden_dim, self.hidden_dim, n_heads, False)
            #l = GCNConv(self.hidden_dim, self.hidden_dim, aggr="max")
            #l = MaxGraphLayer(self.hidden_dim)
            self.layers.append(l)
            self.add_module("layer_edge_type_" + str(j), l)

        # initialise norm
        self.drop = torch.nn.Dropout(dropout)
        self.norm1 = torch.nn.BatchNorm1d(hidden_dim)
        self.drop1 = torch.nn.Dropout(dropout)
        self.norm2 = torch.nn.BatchNorm1d(hidden_dim)
        self.drop2 = torch.nn.Dropout(dropout)

        self.linear1 = torch.nn.Linear(self.hidden_dim, self.d_inner)
        self.linear2 = torch.nn.Linear(self.d_inner, self.hidden_dim)

    def forward(self, x, edge_index, edge_type):
        def edge_index_for_type(type):
            indices = (edge_type == type)
            index = edge_index[:,indices]
            return index

        def layer(l, x, ei):
            return l(x, ei)

        # edge type attention
        x2 = torch.stack([layer(l, x.view(-1, self.hidden_dim), edge_index_for_type(t)) for t,l in enumerate(self.layers)], axis=0).sum(axis=0)
        x = x + self.drop1(x2)
        x = self.norm1.forward(x.view(-1, self.hidden_dim)).view(x.shape)
        x = self.linear2(self.drop(torch.relu(self.linear1(x))))
        x = self.norm2(x.view(-1, self.hidden_dim)).view(x.shape)

        return x

class SimpleGATLayer(torch.nn.Module):
    """
    çº¯ GAT å±‚ï¼Œä¸åŒºåˆ†è¾¹ç±»å‹
    å¯¹æ‰€æœ‰è¾¹ä½¿ç”¨åŒä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶
    """

    def __init__(self, hidden_dim, d_inner, n_heads=8, dropout=0.2):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.d_inner = d_inner

        # åªåˆ›å»ºä¸€ä¸ª GAT å±‚ï¼Œå¤„ç†æ‰€æœ‰è¾¹
        self.gat_layer = GATConv(self.hidden_dim, self.hidden_dim, n_heads, concat=False)

        # Normalization å’Œ FFNï¼ˆä¿æŒå’ŒåŸæ¥ä¸€æ ·ï¼‰
        self.drop = torch.nn.Dropout(dropout)
        self.norm1 = torch.nn.BatchNorm1d(hidden_dim)
        self.drop1 = torch.nn.Dropout(dropout)
        self.norm2 = torch.nn.BatchNorm1d(hidden_dim)
        self.drop2 = torch.nn.Dropout(dropout)

        self.linear1 = torch.nn.Linear(self.hidden_dim, self.d_inner)
        self.linear2 = torch.nn.Linear(self.d_inner, self.hidden_dim)

    def forward(self, x, edge_index):
        """
        æ³¨æ„ï¼šè¿™é‡Œç§»é™¤äº† edge_type å‚æ•°
        """
        # GAT æ³¨æ„åŠ›ä¼ æ’­ï¼ˆæ‰€æœ‰è¾¹ä¸€èµ·å¤„ç†ï¼Œä¸åŒºåˆ†ç±»å‹ï¼‰
        x2 = self.gat_layer(x.view(-1, self.hidden_dim), edge_index)

        # æ®‹å·®è¿æ¥ + LayerNorm
        x = x + self.drop1(x2)
        x = self.norm1.forward(x.view(-1, self.hidden_dim)).view(x.shape)

        # FFN
        x = x + self.drop2(self.linear2(self.drop(torch.relu(self.linear1(x)))))
        x = self.norm2(x.view(-1, self.hidden_dim)).view(x.shape)

        return x

# Their GraphTransformer is learning: "how the network works":
# 1) Structural relationships in network topology: which routers are connected and what are the types of connections;
# 2) Propagation patterns of routing protocols: how BGP/OSPF information propagates within the network;
# 3) Dependencies between configuration parameters: how a router's OSPF weight affects the routing decisions of other routers;
# 4) Global constraints required by the specification: how various parameters should be coordinated to meet reachability requirements;
class EdgeTypeGraphTransformer(torch.nn.Module):
    def __init__(self, hidden_dim, num_layers, num_edge_types):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.propagation_layers = [EdgeTypeTransformerLayer(hidden_dim, 4*hidden_dim, num_edge_types, 8) for i in range(num_layers)]
        for i, l in enumerate(self.propagation_layers):
            self.add_module("prop_layer_" + str(i), l)

    def forward(self, x, edge_index, edge_type):
        for l in self.propagation_layers:
            x = l.forward(x, edge_index, edge_type)
            x = torch.relu(x)
        return x

class SimpleGATEncoder(torch.nn.Module):
    """
    çº¯ GAT ç¼–ç å™¨ï¼Œå †å å¤šå±‚ SimpleGATLayer
    ä¸ä½¿ç”¨è¾¹ç±»å‹ä¿¡æ¯
    """

    def __init__(self, hidden_dim, num_layers):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.propagation_layers = [
            SimpleGATLayer(hidden_dim, 4 * hidden_dim, n_heads=8)
            for i in range(num_layers)
        ]

        for i, l in enumerate(self.propagation_layers):
            self.add_module("prop_layer_" + str(i), l)

    def forward(self, x, edge_index):
        """
        æ³¨æ„ï¼šè¿™é‡Œç§»é™¤äº† edge_type å‚æ•°
        """
        for l in self.propagation_layers:
            x = l.forward(x, edge_index)  # ä¸å†ä¼ é€’ edge_type
            x = torch.relu(x)
        return x

# Encode the original features of network nodes (router ID, IP address, protocol parameters, etc.) into high-dimensional vectors
# this class should be the "EMB function" in paper's Section 4.2
class PredicateGraphEmbedding(torch.nn.Module):
    def __init__(self, features, hidden_dim, num_edge_types, excluded_feature_indices=set(), num_layers=6):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.num_edge_types = num_edge_types

        self.encoder = NodeFeatureEmbedding(hidden_dim, features, excluded_feature_indices=excluded_feature_indices)
        self.decoder = NodeFeatureDecoder(hidden_dim, features, excluded_feature_indices=excluded_feature_indices)

    def forward(self, x, mask, edge_index, edge_type, reliable_masking):
        return self.encoder.forward(x, mask, reliable_masking, positional_encoding=False)

class Model_fd(torch.nn.Module): # maybe compare with the original "class Model()" in "synthesis task" to understand this code
    def __init__(self, features, hidden_dim, num_edge_types, excluded_feature_indices):
        super().__init__()

        self.hidden_dim = hidden_dim # hidden dimension of the neural network controls the model capacity
        self.num_edge_types = num_edge_types # The number of edge types in the graph (4 in the code)

        self.embedding = PredicateGraphEmbedding(features, hidden_dim, self.num_edge_types, excluded_feature_indices) # Create a node embedding layer to convert the original features into high-dimensional vectors

        self.transformer_encoder = EdgeTypeGraphTransformer(self.hidden_dim, 2, num_edge_types)

        # C sompared with the synthesis tasks, we delete all the decoders

        # In the following, we define the fault classifier, which is a simple multilayer perceptron.
            # Input: The overall representation vector of the graph.
            # Output: Scores (logits) for the seven fault classes.
        self.fault_classifier = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim // 2),   # Fully connected layer: 128 -> 64
            torch.nn.ReLU(),                                # Activation Function
            torch.nn.Dropout(0.2),                          # To prevent overfitting, randomly discard 20%
            torch.nn.Linear(hidden_dim // 2, 7)             # Final classification layer: 64 -> 7 fault categories
        )

    def forward(self, x, edge_index, edge_type, batch_idx): # this "forward" is for classification task
        # x:            Node feature matrix [number of nodes, number of features]
        # edge_index:   Edge connection relationship [2, number of edges]
        # edge_type:    The type of each edge [number of edges]
        # batch_idx:    Which graph each node belongs to [number of nodes] (due to batch processing)

        # if edge_index.dim() == 3: # If edge_index is 3-dimensional (batch dimension), take the first batch
        #     edge_index = edge_index[0]
        #     edge_type = edge_type[0]

        # if x.dim() == 2: # If x is 2D [number of nodes, number of features], it is expanded to 3D [number of nodes, 1, number of features]; This is because the embedding layer expects 3D input.
        #     x = x.unsqueeze(1)

        # if edge_index.dim() == 3:
        #     if edge_index.size(0) != 1:
        #         raise ValueError("Batched edge_index not supported yet")
        #     edge_index = edge_index[0]
        #     edge_type = edge_type[0]
        # assert x.size(1) == 1, f"Expected sequence length 1, got {x.size(1)}"

        mask = torch.zeros_like(x, dtype=torch.bool) # åˆ›å»ºä¸€ä¸ªå…¨é›¶çš„maskï¼ˆè¡¨ç¤ºä¸é®ç›–ä»»ä½•ç‰¹å¾ï¼‰
        x = self.embedding.forward(x, mask, edge_index, edge_type, True)[:, 0] # ç‰¹å¾ç¼–ç 
        # [:, 0] removes the dimension just added, changing from [number of nodes, 1, hidden_dim] back to [number of nodes, hidden_dim]
        # The final True parameter indicates the use of a reliable masking method

        x = self.transformer_encoder.forward(x, edge_index, edge_type) # å›¾æ¨ç†
        # Core Steps: Inference via Graph Transformers
        # Let each node's representation incorporate information from neighboring nodes
        # Learn dependencies in the network topology

        graph_representation = global_mean_pool(x, batch_idx) # Graph-level pooling
        fault_logits = self.fault_classifier(graph_representation) # Fault classification

        return fault_logits

class Model_fd_classical_GAT(torch.nn.Module):
    """
    ä½¿ç”¨ç»å…¸ GAT çš„æ•…éšœè¯Šæ–­æ¨¡å‹ï¼ˆbaselineï¼‰
    ä¸ä½¿ç”¨è¾¹ç±»å‹ä¿¡æ¯
    """

    def __init__(self, features, hidden_dim, num_edge_types, excluded_feature_indices):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.num_edge_types = num_edge_types  # ä¿ç•™è¿™ä¸ªå‚æ•°ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰ï¼Œä½†ä¸ä½¿ç”¨

        # Embedding å±‚ï¼ˆä¿æŒä¸å˜ï¼‰
        self.embedding = PredicateGraphEmbedding(
            features, hidden_dim, self.num_edge_types, excluded_feature_indices
        )

        # ä½¿ç”¨ç®€å•çš„ GAT ç¼–ç å™¨ï¼ˆä¸åŒºåˆ†è¾¹ç±»å‹ï¼‰
        self.transformer_encoder = SimpleGATEncoder(self.hidden_dim, num_layers=2)

        # æ•…éšœåˆ†ç±»å™¨ï¼ˆä¿æŒä¸å˜ï¼‰
        self.fault_classifier = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim // 2),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.2),
            torch.nn.Linear(hidden_dim // 2, 7)
        )

    def forward(self, x, edge_index, edge_type, batch_idx):
        """
        æ³¨æ„ï¼šä¿ç•™ edge_type å‚æ•°ï¼ˆä¸ºäº†å…¼å®¹è®­ç»ƒå¾ªç¯ï¼‰ï¼Œä½†åœ¨å†…éƒ¨å¿½ç•¥å®ƒ
        """
        # èŠ‚ç‚¹ç‰¹å¾ç¼–ç 
        mask = torch.zeros_like(x, dtype=torch.bool)
        x = self.embedding.forward(x, mask, edge_index, edge_type, True)[:, 0]
        # æ³¨æ„ï¼šembedding å†…éƒ¨å¯èƒ½è¿˜éœ€è¦ edge_typeï¼Œæ‰€ä»¥è¿™é‡Œè¿˜æ˜¯ä¼ é€’

        # å›¾æ¨ç†ï¼ˆä¸ä½¿ç”¨ edge_typeï¼‰
        x = self.transformer_encoder.forward(x, edge_index)  # å…³é”®ï¼šä¸ä¼ é€’ edge_type

        # å›¾çº§åˆ«æ± åŒ–
        graph_representation = global_mean_pool(x, batch_idx)

        # æ•…éšœåˆ†ç±»
        fault_logits = self.fault_classifier(graph_representation)

        return fault_logits

# def generate_fake_fault_labels(batch): # Temporarily generate a fake fault label
#     batch_size = batch.num_graphs   # PyG è‡ªå¸¦çš„å±æ€§ï¼Œè¡¨ç¤ºbatché‡Œæœ‰å¤šå°‘ä¸ªgraph
#     return torch.randint(0, 7, (batch_size,), device=batch.x.device)

def inject_single_graph_fault_and_label(batch, feature, num_parameters=7, device='cpu'):
    """
    batch: PyG Data batch
    feature: prog.feature_registry.feature
    num_parameters: æ€»å…±å¯å‡ºé”™å‚æ•°æ•°é‡
    """
    batch_size = batch.num_graphs
    labels = torch.randint(0, num_parameters, (batch_size,), device=device)  # æ¯ä¸ªå›¾éšæœºé€‰æ‹©ä¸€ä¸ªå‚æ•°å‡ºé”™

    # å¯¹æ¯ä¸ªå›¾è¿›è¡Œæ³¨å…¥
    for i in range(batch_size):
        fault_idx = labels[i].item() # å“ªä¸ªå‚æ•°å‡ºé”™
        # æ ¹æ® index æ‰¾åˆ° feature å¯¹åº”çš„å¼ é‡åˆ—
        if fault_idx == 0:
            col_idx = feature("predicate_connected_arg2").idx
        elif fault_idx == 1:
            col_idx = feature("predicate_bgp_route_arg2").idx
        elif fault_idx == 2:
            col_idx = feature("predicate_bgp_route_arg3").idx
        elif fault_idx == 3:
            col_idx = feature("predicate_bgp_route_arg4").idx
        elif fault_idx == 4:
            col_idx = feature("predicate_bgp_route_arg5").idx
        elif fault_idx == 5:
            col_idx = feature("predicate_bgp_route_arg6").idx
        elif fault_idx == 6:
            col_idx = feature("predicate_bgp_route_arg7").idx
        else:
            raise ValueError("Invalid fault index")

        if i == 0:
            start_idx = 0
        else:
            start_idx = torch.sum(batch.batch < i).item()
        end_idx = torch.sum(batch.batch <= i).item()

        # åªä¿®æ”¹å±äºç¬¬iä¸ªå›¾çš„èŠ‚ç‚¹
        x_orig = batch.x[start_idx:end_idx, :, col_idx]
        batch.x[start_idx:end_idx, :, col_idx] = x_orig + torch.randint(1, 5, x_orig.shape, device=device)

    return batch.x, labels

def eval_fd(model, dataset, features, num_parameters, step_writer, prefix, device):
    batch_size = 2
    loader = DataLoader(dataset, batch_size=batch_size)

    model.eval()
    val_loss = 0
    val_correct = 0
    val_total = 0
    criterion = torch.nn.CrossEntropyLoss()

    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            batch.x = batch.x.unsqueeze(1)
            batch.edge_index = reflexive(bidirectional(batch.edge_index), num_nodes=batch.x.size(0))
            batch.edge_type = reflexive_bidirectional_edge_type(batch.edge_type, batch.x.size(0))

            # === å’Œè®­ç»ƒä¸€è‡´ï¼šæ³¨å…¥ä¸€ä¸ª fault å¹¶å¾—åˆ°æ ‡ç­¾ ===
            batch.x, fault_labels = inject_single_graph_fault_and_label(batch, features, num_parameters=num_parameters, device=device)

            fault_predictions = model.forward(batch.x, batch.edge_index, batch.edge_type, batch.batch)

            val_loss += criterion(fault_predictions, fault_labels).item()
            _, predicted = torch.max(fault_predictions.data, 1)
            val_total += fault_labels.size(0)
            val_correct += (predicted == fault_labels).sum().item()

    val_accuracy = 100 * val_correct / val_total if val_total > 0 else 0
    step_writer.add_scalar(f"{prefix}/Loss", val_loss / len(loader))
    step_writer.add_scalar(f"{prefix}/Accuracy", val_accuracy)

    print(f"{prefix}: Loss = {val_loss / len(loader):.4f}, Accuracy = {val_accuracy:.2f}%")

def eval_fd_detailed(model, dataset, features, num_parameters, step_writer, prefix, device):
    """
    æ”¹è¿›ç‰ˆçš„è¯„ä¼°å‡½æ•°ï¼Œä¿å­˜æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
    """
    batch_size = 2
    loader = DataLoader(dataset, batch_size=batch_size)

    model.eval()
    criterion = torch.nn.CrossEntropyLoss(reduction='none')  # â† æ”¹æˆ 'none'ï¼Œè¿”å›æ¯ä¸ªæ ·æœ¬çš„loss

    # ç”¨äºå­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ç»“æœ
    all_losses = []  # æ¯ä¸ªæ ·æœ¬çš„loss
    all_predictions = []  # æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ
    all_labels = []  # æ¯ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾
    all_correct = []  # æ¯ä¸ªæ ·æœ¬æ˜¯å¦é¢„æµ‹æ­£ç¡® (True/False)

    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            batch.x = batch.x.unsqueeze(1)
            batch.edge_index = reflexive(bidirectional(batch.edge_index), num_nodes=batch.x.size(0))
            batch.edge_type = reflexive_bidirectional_edge_type(batch.edge_type, batch.x.size(0))

            # æ³¨å…¥æ•…éšœ
            batch.x, fault_labels = inject_single_graph_fault_and_label(
                batch, features, num_parameters=num_parameters, device=device)

            # é¢„æµ‹
            fault_predictions = model.forward(batch.x, batch.edge_index, batch.edge_type, batch.batch)

            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„lossï¼ˆä¸æ±‚å’Œï¼‰
            losses = criterion(fault_predictions, fault_labels)  # shape: [batch_size]

            # è·å–é¢„æµ‹ç»“æœ
            _, predicted = torch.max(fault_predictions.data, 1)

            # åˆ¤æ–­æ˜¯å¦é¢„æµ‹æ­£ç¡®
            correct = (predicted == fault_labels)  # shape: [batch_size], bool tensor

            # ä¿å­˜åˆ°åˆ—è¡¨ï¼ˆè½¬æ¢ä¸ºnumpyä¾¿äºåç»­å¤„ç†ï¼‰
            all_losses.extend(losses.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(fault_labels.cpu().numpy())
            all_correct.extend(correct.cpu().numpy())

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_losses = np.array(all_losses)
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_correct = np.array(all_correct)

    # è®¡ç®—ç»Ÿè®¡é‡
    mean_loss = all_losses.mean()
    std_loss = all_losses.std()
    accuracy = 100 * all_correct.sum() / len(all_correct)

    # æ‰“å°ç»“æœ
    print(f"\n{'=' * 60}")
    print(f"{prefix} Results:")
    print(f"  Mean Loss: {mean_loss:.4f} Â± {std_loss:.4f}")
    print(f"  Accuracy:  {accuracy:.2f}% ({all_correct.sum()}/{len(all_correct)})")
    print(f"  Min Loss:  {all_losses.min():.4f}")
    print(f"  Max Loss:  {all_losses.max():.4f}")
    print(f"{'=' * 60}\n")

    # å†™å…¥TensorBoard
    step_writer.add_scalar(f"{prefix}/Loss_Mean", mean_loss)
    step_writer.add_scalar(f"{prefix}/Loss_Std", std_loss)
    step_writer.add_scalar(f"{prefix}/Accuracy", accuracy)

    # è¿”å›è¯¦ç»†ç»“æœ
    results = {
        'losses': all_losses,  # [100] æ¯ä¸ªæ ·æœ¬çš„loss
        'predictions': all_predictions,  # [100] æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹
        'labels': all_labels,  # [100] æ¯ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾
        'correct': all_correct,  # [100] æ¯ä¸ªæ ·æœ¬æ˜¯å¦æ­£ç¡®
        'mean_loss': mean_loss,
        'std_loss': std_loss,
        'accuracy': accuracy
    }

    return results


# ====================================================================
# æ‰¹é‡è¯„ä¼°å‡½æ•° - ä¸“é—¨ä¸º Classical GAT è®¾è®¡
# ====================================================================

def batch_evaluate_classical_gat_models():
    """æ‰¹é‡è¯„ä¼°æ‰€æœ‰ Classical GAT æ¨¡å‹æ£€æŸ¥ç‚¹"""

    # 1. æŸ¥æ‰¾æ‰€æœ‰ Classical GAT æ¨¡å‹æ–‡ä»¶ï¼ˆæ³¨æ„æ–‡ä»¶åæ ¼å¼ï¼‰
    model_pattern = "models/FD-GAT2018-epoch*.pt"
    model_files = sorted(glob.glob(model_pattern))

    if not model_files:
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼è¯·æ£€æŸ¥è·¯å¾„: {model_pattern}")
        print(f"   å½“å‰ç›®å½•: {os.getcwd()}")
        print(f"   models ç›®å½•å†…å®¹:")
        if os.path.exists("models"):
            for f in os.listdir("models")[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
                print(f"     - {f}")
        return None, None

    print(f"âœ… æ‰¾åˆ° {len(model_files)} ä¸ª GAT 2018 æ¨¡å‹æ–‡ä»¶")

    # 2. æå– epoch ç¼–å·
    def extract_epoch(filename):
        match = re.search(r'epoch(\d+)', filename)
        return int(match.group(1)) if match else -1

    model_files = sorted(model_files, key=extract_epoch)

    # 3. åˆ›å»ºç»“æœå­˜å‚¨ç›®å½•
    os.makedirs("./results", exist_ok=True)

    # 4. å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {
        'epochs': [],
        'mean_losses': [],
        'std_losses': [],
        'accuracies': []
    }

    # 5. é€‰æ‹©éªŒè¯é›†
    validation_datasets = {
        'origin': validation_dataset_origin,
        # 'larger': validation_dataset_larger,
        'real': validation_dataset_real
    }

    # 6. æ‰¹é‡è¯„ä¼°ï¼ˆå¯¹æ‰€æœ‰ä¸‰ä¸ªæ•°æ®é›†ï¼‰
    print("\n" + "=" * 70)
    print("å¼€å§‹æ‰¹é‡è¯„ä¼° GAT 2018 æ¨¡å‹...")
    print("=" * 70)

    for dataset_name, validation_dataset in validation_datasets.items():
        print(f"\n{'=' * 70}")
        print(f"ğŸ“Š æ­£åœ¨è¯„ä¼°æ•°æ®é›†: {dataset_name.upper()}")
        print(f"{'=' * 70}")

        # é‡ç½®ç»“æœå­˜å‚¨ï¼ˆæ¯ä¸ªæ•°æ®é›†ç‹¬ç«‹ï¼‰
        all_results = {
            'epochs': [],
            'mean_losses': [],
            'std_losses': [],
            'accuracies': []
        }

        for model_file in model_files:
            epoch = extract_epoch(model_file)
            print(f"\n>>> [{dataset_name}] æ­£åœ¨è¯„ä¼°: Epoch {epoch}")

            try:
                # åŠ è½½æ¨¡å‹
                checkpoint = torch.load(model_file, map_location=device)
                model.load_state_dict(checkpoint[0])

                # åˆ›å»º step_writer
                step_writer = StepWriter(writer, epoch)

                # è¯„ä¼°æ¨¡å‹
                results = eval_fd_detailed(
                    model,
                    validation_dataset,
                    feature,
                    NUM_FAULT_CLASSES,
                    step_writer,
                    prefix=f"Val_GAT2018_{dataset_name}_Epoch{epoch}",
                    device=device
                )

                # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆå…³é”®ï¼šæ–‡ä»¶åæ ¼å¼è¦åŒ¹é…ï¼‰
                if dataset_name == 'orign':
                    filename = f"./results/val_GAT2018_epoch{epoch}.npz"
                else:
                    filename = f"./results/val_GAT2018_epoch{epoch}_{dataset_name}.npz"

                np.savez(
                    filename,
                    losses=results['losses'],
                    predictions=results['predictions'],
                    labels=results['labels'],
                    correct=results['correct']
                )

                # æ”¶é›†æ±‡æ€»æ•°æ®
                all_results['epochs'].append(epoch)
                all_results['mean_losses'].append(results['mean_loss'])
                all_results['std_losses'].append(results['std_loss'])
                all_results['accuracies'].append(results['accuracy'])

                print(
                    f"âœ… [{dataset_name}] Epoch {epoch}: Loss={results['mean_loss']:.4f}, Acc={results['accuracy']:.2f}%")

            except Exception as e:
                print(f"âŒ è¯„ä¼° {model_file} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue

        # 7. ä¿å­˜è¯¥æ•°æ®é›†çš„æ±‡æ€»ç»“æœ
        summary_file = f"./results/summary_GAT2018_{dataset_name}.npz"
        np.savez(
            summary_file,
            epochs=np.array(all_results['epochs']),
            mean_losses=np.array(all_results['mean_losses']),
            std_losses=np.array(all_results['std_losses']),
            accuracies=np.array(all_results['accuracies'])
        )
        print(f"\nâœ… [{dataset_name}] æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")

    print("\n" + "=" * 70)
    print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†è¯„ä¼°å®Œæˆï¼")
    print("=" * 70)

    return True

# mainloop of classification task
if __name__ == '__main__':
    def get_args():
        parser = argparse.ArgumentParser()
        parser.add_argument("--hidden-dim", dest="hidden_dim", type=int, default=128)
        parser.add_argument("--epochs",     dest="epochs",     type=int, default=NUM_TRAINING_EPOCH)
        parser.add_argument("--checkpoint", dest="checkpoint", type=str, default=None)
        return parser.parse_args()

    args = get_args()
    HIDDEN_DIM = args.hidden_dim

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("====> Running on", device)
    snapshot = ModelSnapshot(__file__)

    # Data loading
    sem = BgpSemantics()
    # dataset = FactBaseSemanticsDataset(sem, "bgp-ospf-dataset-sub", num_samples=64)
    # print("====> Dataset Size", len(dataset))

    validation_dataset_origin = FactBaseSemanticsDataset(sem, "bgp-ospf-dataset-test-origin", num_samples=NUM_LEN_DATASET_EVAL)
    validation_dataset_larger = FactBaseSemanticsDataset(sem, "bgp-ospf-dataset-test-larger", num_samples=NUM_LEN_DATASET_EVAL)
    validation_dataset_real   = FactBaseSemanticsDataset(sem, "bgp-ospf-dataset-test-real",   num_samples=NUM_LEN_DATASET_EVAL)

    # num_validation_samples = 16
    # training_dataset, validation_dataset = dataset[num_validation_samples:], dataset[:num_validation_samples]

    # print("====> Validation Dataset Size", len(validation_dataset))

    # Feature Settings
    predicate_declarations = sem.decls()
    prog = FactBase(predicate_declarations)
    feature = prog.feature_registry.feature

    excluded_feature_indices = set([1])
    features = prog.feature_registry.get_all_features()
    print(prog.predicate_declarations)
    print(features)

    # === è¿™é‡Œä½ è¦è®¾å®š fault classification çš„ç±»åˆ«æ•°é‡ï¼ˆæ¯”å¦‚ 7ï¼‰ ===
    NUM_FAULT_CLASSES = 7

    # Creating a fault diagnosis model
    # model = Model_fd(features, HIDDEN_DIM, NUM_EDGE_TYPES, excluded_feature_indices).to(device)
    model = Model_fd_classical_GAT(features, HIDDEN_DIM, NUM_EDGE_TYPES, excluded_feature_indices).to(device)
    model.feature = feature

    # Loss function and optimizer of Classification task
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)

    writer = snapshot.writer()
    print("Model parameters:", len(list(model.parameters())))

    # Training parameters
    # batch_size = 4
    # num_samples_per_epoch = 32
    # num_batches_per_epoch = int(num_samples_per_epoch / batch_size)


    # ########################### HERE, do the evaluations ###########################
    # validation_dataset = validation_dataset_origin
    # # validation_dataset = validation_dataset_larger
    # # validation_dataset = validation_dataset_real
    #
    # checkpoint_path = "models/fault_localization-model-classicalGAT-epoch20.pt"  # æ”¹æˆä½ å®é™…çš„æ–‡ä»¶å
    # checkpoint = torch.load(checkpoint_path, map_location=device)
    # model.load_state_dict(checkpoint[0])
    #
    # step_writer = StepWriter(writer, NUM_TRAINING_EPOCH-1)
    #
    # # eval_fd(model, validation_dataset, feature, NUM_FAULT_CLASSES, step_writer, prefix="Val", device=device)
    # results = eval_fd_detailed(model, validation_dataset, feature, NUM_FAULT_CLASSES, step_writer, prefix="Val", device=device)
    #
    # # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
    # epoch = NUM_TRAINING_EPOCH
    # np.savez(f"./results/val_classicalGAT_epoch{epoch}.npz",
    #          losses=results['losses'],
    #          predictions=results['predictions'],
    #          labels=results['labels'],
    #          correct=results['correct'])
    # æ‰¹é‡è¯„ä¼°æ‰€æœ‰ Classical GAT æ¨¡å‹
    batch_evaluate_classical_gat_models()

    print("\nâœ… 2018 GAT æ‰¹é‡è¯„ä¼°å®Œæˆï¼")



    # print("Evaluation completed!")


